{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Linear Regression with Gradient Descent and Regularization\n",
    "\n",
    "In this task, your will explore linear regression with gradient descent and regularization.\n",
    "Previously we performed linear regression but we actually were using the built-in \"normal\" equations which automagically were solving for coefficients.  In this lab we will use scikit-learns SGDRegressor, which stands for Stochastic Gradient Descent Regression -- using Gradient descent to perform linear regression! \n",
    "\n",
    "Our goals are two fold\n",
    "\n",
    "1. Practice setting the learning rate on SGD\n",
    "2. Practice setting the regularization term with linear regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the things we need\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Synthetic Data.\n",
    "We will use our handy functions again to create data.\n",
    "\n",
    "* Experiment with the **`noise`** keyword argument to see how it affects the graph below, after you are done experimenting, set `noise=20`.\n",
    "* Experiment with the **`random_state`** variable to get different sets of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=100, n_features=1, noise = 20.0, random_state = 10)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create testing data to plot our prediction onto later - \n",
    "# this code is how we \"plot a line !!\" we need data to create the line\n",
    "X_test = np.linspace(min(X),max(X),100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with Gradient Descent\n",
    "Ok, now we'll let the computer \"learn\" for itself what the best line is, but this time using gradient descent.\n",
    "We'll use the [SGDRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor) model from scikit-learn to do this.  `SGDRegressor` implements Stochastic Gradient Descent and we will set it up to use a least squares (mean squared error) loss function. \n",
    "\n",
    "Let's create a linear regression model and fit it to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use our plot_linear function from before, except this time we will pass a built model into it.\n",
    "def plot_linear(model):\n",
    "    model.fit(X,y)\n",
    "    print (\"M :  {}, C : {}\".format(model.coef_, model.intercept_))\n",
    "    y_test = model.predict(X_test.reshape(-1,1))\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.scatter(X,y)\n",
    "    plt.plot(X_test.flatten(), y_test)\n",
    "    plt.title(\"mean squared error: {0:.3g}\".format(mean_squared_error(model.predict(X), y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's review the parameters to set:\n",
    "\n",
    "* `loss`:  This is the \"loss\" or \"cost\" function from the lectures. I am explicitly setting it to \"squared_error\" which is the exact same as MSE from lecture. Note that SGD actually defaults to this (I could have left it blank), but I wanted to show it to you so we are setting it manually.\n",
    "* `penalty`: 'l2' this is the kind of regularization penalty we want to apply.  L2 is a mathematical distance, it's the euclidean distance (the kind we are all used to).\n",
    "* `alpha`: this is the regularization term penalty, it is our knob for regularization. We will try many different sizes here. \n",
    "* `max_iter`: this is how many times we want to run gradient descent.\n",
    "* `learning_rate` : this is the learning rate _schedule_ which is something we have not learned yet, but basically it's common to change your learning rate over time.  There are some default optionsn we will leave it on \"constant\" though, since we want to try out the most basic version of gradient descent.\n",
    "* `eta0` : this is the actual number that will be used as the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are all the default parameters\n",
    "sgd = SGDRegressor(loss='squared_error',\n",
    "                   penalty= 'l2',\n",
    "                   alpha = 0.0001,\n",
    "                   max_iter = 1000,\n",
    "                   learning_rate = \"constant\",\n",
    "                   eta0 = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_linear(sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So with all defaults our SGDRegressor performs very well.  Your job now is to try and _break_ our SGDRegressor.  What happens if you use a learning rate that is too small?  Too large?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set eta0 very small here\n",
    "sgd = SGDRegressor(loss='squared_error',\n",
    "                   penalty= 'l2',\n",
    "                   alpha = 0.0001,\n",
    "                   max_iter = 1000,\n",
    "                   learning_rate = \"constant\",\n",
    "                   eta0 = )\n",
    "plot_linear(sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set eta0 very large here\n",
    "sgd = SGDRegressor(loss='squared_error',\n",
    "                   penalty= 'l2',\n",
    "                   alpha = 0.0001,\n",
    "                   max_iter = 1000,\n",
    "                   learning_rate = \"constant\",\n",
    "                   eta0 = )\n",
    "plot_linear(sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go ahead and play with the parameters a bit.  You can try different settings for max_iter and eta0.  In the next section we will look at how alpha, the regularization parameter affects a polynomial regressor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Regression\n",
    "\n",
    "Now let's give our regression model more degrees of freedom, this will let us learn how to set the regularization parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def plot_poly(sgd, degree = 3):\n",
    "    # make a pipeline that creates the polynomial features based on our input data\n",
    "    # this is akin to using performing polynomial regression\n",
    "    # see http://scikit-learn.org/stable/modules/linear_model.html#polynomial-regression-extending-linear-models-with-basis-functions\n",
    "    \n",
    "    model = Pipeline([('poly', PolynomialFeatures(degree=degree)),\n",
    "                       ('linear', sgd)])\n",
    "    model.fit(X,y)\n",
    "    y_test = model.predict(X_test)\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.scatter(X,y)\n",
    "    plt.plot(X_test.flatten().reshape(-1,1), y_test, color = 'teal')\n",
    "    plt.title(\"mean squared error: {0:.3g}\".format(mean_squared_error(model.predict(X), y)))\n",
    "    plt.ylim((min(y)-10,max(y)+10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting Alpha (regularization) parameter.\n",
    "\n",
    "In this section we are going to use the Ridge regression model from scikilt-learn.  Ridge Regression will do the same regression, except with statistical methods that are different from gradient descent.  The reason we're using Ridge regression is that it behaves better when the dataset is smaller. The SGDRegressor would work (you can try it!) but it struggles to find a \"good\" fit when the dataset is so small (you will notice this if you try it out with any degree larger than 3).  Therefore since we want to get an intuition about regularization we'll practice regularization on the [Ridge Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge) model from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no regularization!\n",
    "ridge_ = Ridge(alpha = 0.0)\n",
    "plot_poly(ridge_, degree = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.0 regularization!\n",
    "ridge_ = Ridge(alpha = 1.0)\n",
    "plot_poly(ridge_, degree = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens with the interaction between degrees of freedom and a small alpha vs a large alpha?\n",
    "\n",
    "If `alpha` is very small then the regularizatiom penalty is small, which would allow a lot of variance (lead to overfitting), if `alpha` is large, then the penalty will be large and the model will have a lot of bias (underfitting).\n",
    "\n",
    "Try out different ranges of `alpha`, I would try all kinds of values from `0.001, 10000.0`\n",
    "How does `alpha` interact with the degree? Set large degrees and small degrees.\n",
    "\n",
    "Go ahead and try out different values and see what relationship you learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you try!\n",
    "ridge_ = Ridge(alpha = )\n",
    "plot_poly(ridge_, degree = )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you try!\n",
    "ridge_ = Ridge(alpha = )\n",
    "plot_poly(ridge_, degree = )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you try!\n",
    "ridge_ = Ridge(alpha = )\n",
    "plot_poly(ridge_, degree = )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you try!\n",
    "ridge_ = Ridge(alpha = )\n",
    "plot_poly(ridge_, degree = )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you try!\n",
    "ridge_ = Ridge(alpha = )\n",
    "plot_poly(ridge_, degree = )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you try!\n",
    "ridge_ = Ridge(alpha = )\n",
    "plot_poly(ridge_, degree = )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
