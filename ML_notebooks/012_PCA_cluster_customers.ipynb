{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "\n",
    "#preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from scipy.stats import mode\n",
    "\n",
    "\n",
    "# pipelines\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Data\n",
    "\n",
    "In this assignment we are going to examine some customer data gathering from my very famous internet company \"the best one ever\".  \"The best one ever\" is the best company ever that sells important things online.  In this investigation we want to find if there are any natural groups of customers in my dataset.  The first step is to just get the data in a format we can feed to our machine learning models. Once we do that, then our boss (some dude named Gilad), said he will teach us how to cluster the customers! But it turns out you need to have the _data_ formatted in some special way...? Maybe you can tell me about that!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tips\n",
    "\n",
    "* It's a perfectly good idea to just open the CSV file up with excel, but maybe easier to just open it in jupyter lab. Lab has a very good CSV reader, which will allow you to \"look\" around very easily.\n",
    "\n",
    "* When loading the dataset, you may need to pay special attention to where the index is and tell pandas where it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data found in \"best_one_ever_database.csv\" and take a look at the head and info.\n",
    "data = pd.read_csv(\"best_one_ever_database.csv\", index_col = 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning The Data\n",
    "\n",
    "Taking a look at the data you have to ask yourself the questions\n",
    "\n",
    "1. 'Which columns are useful for me to keep?'\n",
    "2. 'Are all the columns usable as features?\n",
    "\n",
    "Then you may have to do some \"work\" to get the column to be usable. Let's look at one column together. The first column is titled \"first_name\" and it seems to be the first name of each customer. Is this a usable feature? Well... not exactly in string format. So I guess I could one hot encode them into binary vectors, but even then... do I want to cluster the customers based on their first name? You can imagine some situation where clustering by name might be relevant (for example trying to guess what generation someone belonged to?) but in this case it seems like it's more of a unique identifier so it may be best to simply remove it. If every value in a column is unique (there are no duplications of the value) then we shouldn't use it as a feature because it will have a 1-1 mapping with the target variable which is not something we ever want. We want our model to learn and generalize from the features, not memorize that the name \"jane\" bought 5 cans of soda.\n",
    "\n",
    "Ok, that's the first column, we vote drop! Now you have to go through each and every column and ask yourself \"do I keep it? if yes, what extra work might I have to do?\" \n",
    "\n",
    "Let's walk through it\n",
    "  \n",
    "  1. first_name:  this is a unique identifier so we should remove it.\n",
    "  2. last_name: see above\n",
    "  3. email: this is unique to an extent.  BUT if we strip the name@ portion of the email and simply keep the domain name, it could possibly aid us. Perhaps certain kinds of customers use certain email services! Worth looking into\n",
    "  4. Gender: this is certainly relevant, but it's categorical data. We will need to one-hot-encode it.\n",
    "  5. ip_address: We can perhaps segment the ip's into fields and use them, there maybe overlaps or correlations among different fields. Or maybe you know more about IP addresses than I do and this is totally useless\n",
    "  6. sales: we certainly need this column, but we need to convert it a floating point type: remove the '$' and convert the dtype of the column\n",
    "  7. zip_code: I think we can just leave this as is.\n",
    "  8. prob-of_rebuy: I think we can just leave this as is.\n",
    "  9. Money_spent: seems fine to me!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the name columns using pandas.drop()\n",
    "\n",
    "X = data.drop(labels = ['first_name','last_name'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform string columns into useful features\n",
    "\n",
    "1. email column\n",
    "2. sales column (remove $ sign)\n",
    "\n",
    "We will use the pandas `apply` function take a function that operates on a string and apply it to the entire column. I will do the first one, and you will do the next one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_dollar(x):\n",
    "    return x[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function to the column and assign it back to the column (it does not work inplace)\n",
    "X.sales = X.sales.apply(strip_dollar)\n",
    "\n",
    "# cast the column to a floating point type - this is very important, otherwise it will be\n",
    "# an object type column that we cannot do arithmetic on the column\n",
    "X.sales = X.sales.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn\n",
    "\n",
    "Now you need to\n",
    "1. write a function to strip the name portion of the email\n",
    "2. Apply it to the column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions to apply to the dataframe\n",
    "def strip_emails(x):\n",
    "    at = x.find('@')\n",
    "    return x[at+1:]\n",
    "\n",
    "test_email = 'thisismymail@gmail.com'\n",
    "print(strip_emails(test_email))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function to the column and assign it back to the column (it does not work inplace)\n",
    "X.email = X.email.apply(strip_emails)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is the email column going to be worth it?\n",
    "Let's take a look at this email column and decide if it could help us or not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many unique domains are there?\n",
    "counts = X.email.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(counts.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, it doesn't seem like this column will be very helpful as it's quite spread out. There are 490 unique values and no one value has a majority, so let's just leave it to the side for now. We can always come back to it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we make sure to copy it over and save it for later.\n",
    "emails = X.email.copy()\n",
    "X.drop('email', axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the IP Address\n",
    "We now need to split up the IP address, we will use Pandas's built in str method for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[['first_ip','second_ip','third_ip','fourth_ip']] = X.ip_address.str.split(pat=\".\", expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we cast the columns as floats\n",
    "X[['first_ip','second_ip','third_ip','fourth_ip']] = X[['first_ip','second_ip','third_ip','fourth_ip']].astype('float32')\n",
    "# we also drop the original column\n",
    "X.drop('ip_address', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Encoding\n",
    "\n",
    "Ok we are almost done, we just have to convert the gender column into something integer that we can use. We will use one-hot-encoding since gender is a categorical variable.\n",
    "\n",
    "Pandas has a `get_dummies()` function that will be very useful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.gender.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumbdumbs = pd.get_dummies(X['gender'])\n",
    "X= pd.concat((X,dumbdumbs), axis = 1)\n",
    "X.drop(['gender'], axis = 1, inplace=True)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing Stage 2 - Missing values\n",
    "Ok, we are done with stage 1 (we have converted everything into numeric features and dropped all the unneccessary things.\n",
    "\n",
    "However we do have missing values. Which two columns have missing values?\n",
    "How many values are missing?\n",
    "What should we do about those missing values?\n",
    "\n",
    "You can either impute (fill in) the missing values, or drop the rows which contain them. The choice is up to you!\n",
    "Either way, you should practice both methods. This way you can practice coding both solutions.\n",
    "\n",
    "Note:\n",
    "The `DataFrame.fillna()` method essentially assumes that you are using timeseries data. We are not, so I wouldn't use this. In order to impute simple values, you can use numpy easily, but... I'm lazy and would probably use the scikit-learn implementation.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/impute.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = imp.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= pd.DataFrame(X_, columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "Ok, now we have our customer data all ready to go. We have done all the hard work of preprocessing. Let's feed this data into our algorithms!\n",
    "\n",
    "We'll try two different clustering algorithms. K-means and DB-scan.\n",
    "\n",
    "\n",
    "Our goal is to cluster the data and learn what kinds of customers we have, are they related to one another at all? In order to do this we should try some clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means\n",
    "\n",
    "Let's just make some clusters and evaluate it with their silhouette score. A reminder about the silhouette score\n",
    "\n",
    ">The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters. Negative values generally indicate that a sample has been assigned to the wrong cluster, as a different cluster is more similar.\n",
    "\n",
    "From : https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score\n",
    "\n",
    "Your Job:\n",
    "Run a k-means loop on clusters 2-n (you decide how many you want to try!) and see which # of clusters is best.\n",
    "\n",
    "You should scale the data first since we are hunting for clusters we definitely want the dimensions to be on the same scale (remember that distance means everything here!).\n",
    "\n",
    "#### Note:\n",
    "In an effort to slowly remove your training wheels, I have not important everything you need to accomplish your tasks here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "\n",
    "# clustering metrics\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale your data with a scaler of your choice\n",
    "ss = StandardScaler()\n",
    "x_scaled = ss.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random guess for k\n",
    "kmeans = KMeans(n_clusters = 10)\n",
    "kmeans.fit(x_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = kmeans.predict(x_scaled)\n",
    "print(silhouette_score(x_scaled, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2,20):\n",
    "    kmeans = KMeans(n_clusters = i)\n",
    "    kmeans.fit(x_scaled)\n",
    "    labels = kmeans.predict(x_scaled)\n",
    "    print(f\"The number of clusters is {i} and the Silhouette score is {silhouette_score(x_scaled, labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Discussion\n",
    "\n",
    "Ok, so we found the best number of clusters according to the silhouette score.  So what? I mean to say, what do we do with that? We know that according the silhouette score this is the best, but... even if that's correct, _now what_? How do we use these clusters to help us run our business?\n",
    "How can we learn what these clusters represent?\n",
    "\n",
    "I can think of one thing to check\n",
    "\n",
    "1. Look at the feature values of the cluster centroids.\n",
    "\n",
    "Recall that every cluster in kmeans has a centroid. That centroid is the very _center_ of the cluster, so we can look at the feature values of the centroids and see what they tell us. In theory the centroids represent the cluster (generally). \n",
    "\n",
    "We can look at the cluster centroids right now.  Let's examine then with a barplot.\n",
    "\n",
    "I'm going to give you some code help here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a kmeans cluster with desired number of components\n",
    "kmeans = KMeans(3).fit(x_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look for the centers of your clusters.  \n",
    "# Hint: your kmeans object has an attribute you'd want to use\n",
    "centers = kmeans.cluster_centers_\n",
    "print(kmeans.cluster_centers_.shape)\n",
    "print(kmeans.cluster_centers_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my gift to you\n",
    "def plot_centroids(centers):\n",
    "    pd.DataFrame(centers, columns = X.columns).plot(kind = 'bar', figsize = (12,6))\n",
    "    plt.legend(loc='best', bbox_to_anchor=(0.8, 0.1, 0.5, 0.5)); # bbox is (x,y, width, height)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_centroids(centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine your centroids\n",
    "\n",
    "What does your graph tell you?\n",
    "What features are the most important and contributing towards the clusters?\n",
    "\n",
    "Extra-Credit: Rerun your clustering algorithm with a _different_ scaling method and re-examine your centroid features. Do they change? Why? What does it mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DB-Scan\n",
    "\n",
    "Let's now try DB-scan. Remember we have three parameters we need to set\n",
    "1. `eps` the radius of our circle that we will search in\n",
    "2. `min_samples` the minimum number of samples we need to find within our radius to call it a cluster\n",
    "3. `metric` our distance metric.  Defaults to Euclidean (L2-norm).\n",
    "\n",
    "So, we get to fiddle with 3 parameters, but we don't have to worry about \"how many\" clusters to look for, db-scan will decide for us.\n",
    "\n",
    "Go ahead and run it!\n",
    "What is the best eps / min_samples ?\n",
    "What gets you the best silhouette score?\n",
    "\n",
    "In order to answer these questions we will need to figure out a few things\n",
    "\n",
    "1. run dbscan (that's pretty easy)\n",
    "2. how many clusters did it find?\n",
    "3. how do we get the labels (predictions) from the dbscan object?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run dbscan \n",
    "# pick an eps -- how might we do this? what range are your features in?\n",
    "# Your eps should be relevant to the feature space you exist in\n",
    "# pick min_samples\n",
    "\n",
    "dbscan = DBSCAN(eps=3.0, min_samples=5).fit(x_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### ok, now you fit a dbscan. \n",
    "1. how do we figure out how many clusters it found?\n",
    "DBscan has 3 main attributes, 1\n",
    "\n",
    ">**core_sample_indices_ndarray of shape (n_core_samples,)**\n",
    "Indices of core samples.\n",
    "\n",
    ">**components_ndarray of shape (n_core_samples, n_features)**\n",
    "Copy of each core sample found by training.\n",
    "\n",
    ">**labels_ndarray of shape (n_samples)**\n",
    "Cluster labels for each point in the dataset given to fit(). Noisy samples are given the label -1.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html\n",
    "\n",
    "Go ahead and look at those attributes. Print them out. Which one will help us figure out how many clusters DBscan found?\n",
    "\n",
    "Also pay attention to changing `eps`, if `eps` is too small what happens? Too large?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan.labels_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan.core_sample_indices_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan.core_sample_indices_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the Silhouette Score\n",
    "\n",
    "In order to perform the silhouette score we need the labels (that's pretty easy they are given to us), however we should _exclude_ the -1's because they represent points that are _not_ in a cluster. So they are basically discarded. We should probably collect those somewhere and else and see how large that number is. But it should not be part of the silhouette core.\n",
    "\n",
    "So\n",
    "1. seperate out the points which have the label `-1`\n",
    "\n",
    "Then after we have done that, we can calcluate the silhouette score easily. \n",
    "#### NOTE\n",
    "DBscan in scikit-learn is implemented with the notion of \"core samples\".  From the [documentation:](https://scikit-learn.org/stable/modules/clustering.html#dbscan)\n",
    ">More formally, we define a core sample as being a sample in the dataset such that there exist min_samples other samples within a distance of eps, which are defined as neighbors of the core sample. This tells us that the core sample is in a dense area of the vector space. A cluster is a set of core samples that can be built by recursively taking a core sample, finding all of its neighbors that are core samples, finding all of their neighbors that are core samples, and so on. **A cluster also has a set of non-core samples, which are samples that are neighbors of a core sample in the cluster but are not themselves core samples.** Intuitively, these samples are on the fringes of a cluster.\n",
    "\n",
    "This means that the attributes `components` and `core_sample_indices` will only return core samples, for us to get all the points _within_ the cluster we will have to rely on the labels\n",
    "\n",
    "### Number of clusters?\n",
    "But what about the # of clusters? How many did we find and how many in each cluster? We need to examine the labels again, but this time count the unique ones and also count how many of each unique we have.\n",
    "\n",
    "### Goal\n",
    "Run our little line of code, which gives both the score and # of clusters.\n",
    "\n",
    "`print(f\"The number of clusters is {num_clusters} and the Silhouette score is {silhouette_score(x_scaled, dbscan.labels_)}\")`\n",
    "\n",
    "\n",
    "Oh! we also should probably add\n",
    "`print(f\"The number of discarded points was {}\")`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniques, counts = np.unique(dbscan.labels_, return_counts=True)\n",
    "print(uniques)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan.core_sample_indices_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan.labels_!=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove the -1's from the labels.\n",
    "## you also will need to remove the corresponding x_scaled samples so you can compute the silhouette score.\n",
    "# you can use numpy boolean indexing masks for this.\n",
    "\n",
    "labels = dbscan.labels_[dbscan.labels_!=-1]\n",
    "x_points = x_scaled[dbscan.labels_!=-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ok if you have the labels and x's without -1's you can actually computer the Silhouette score now\n",
    "silhouette_score(x_points, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## how many clusters are there though?\n",
    "## you need to figure out how many unique labels there are, and also it would be nice to know\n",
    "## how many points are in each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  https://www.kite.com/python/answers/how-to-count-frequency-of-unique-values-in-a-numpy-array-in-python\n",
    "\n",
    "uniques, counts = np.unique(labels, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(uniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.asarray((uniques,counts)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = len(np.unique(labels))\n",
    "print(f\"The number of clusters is {num_clusters} and the Silhouette score is {silhouette_score(x_points, labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does it compare?\n",
    "\n",
    "Did your DBscan find similar clusters to k-means? Same number of clusters? More ? Less? What about the silhouette score? Better or worse?\n",
    "\n",
    "\n",
    "### Next we'd like to look at the centroids\n",
    "\n",
    "Actually, DBscan doesn't have centroids naturally the same way k-means does. But we can compute it ourselves.\n",
    "We would need to isolate the points of each group, and then just take the mean column wise, that would represent the centroid of each cluster.  You can do this with a numpy mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = []\n",
    "for label in uniques:\n",
    "    centers.append(x_points[labels==label].mean(axis=0))\n",
    "\n",
    "dbcenters = np.array(centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbcenters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_centroids(dbcenters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do your centroids compare?\n",
    "\n",
    "Did you find similar clusters? Dissimilar?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA\n",
    "Ok now we want to do PCA.\n",
    "We'll do three things\n",
    "\n",
    "1. Plot the cumulative sum of PCA's explained variance ratio, this will help us identify how much information we are losting when we downsample with PCA\n",
    "2. Plot a scatter plot of the PCA data in 2 dimensions\n",
    "3. Look at the PCA loadings, this will tell us which features are contributing to the new PCA dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a PCA object.\n",
    "# fit your PCA object on the scaled data\n",
    "# look at the PCA shape , is it what you expected?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scree_plot(pca):\n",
    "    '''Display a scree plot for the pca'''\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "    \n",
    "    scree = pca.explained_variance_ratio_*100\n",
    "    ax.bar(np.arange(len(scree))+1, scree)\n",
    "    ax.plot(np.arange(len(scree))+1, scree.cumsum(),c=\"red\",marker='o')\n",
    "    ax.set_xlabel(\"Principal components\")\n",
    "    ax.set_ylabel(\"Percentage explained variance\")\n",
    "    ax.set_title(\"Scree plot\")\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot your scree plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The 'elbow' method of cumsum plots\n",
    "\n",
    "With the elbow method of a cumulative sum plot (cumsum), we want to find the point where we see a drop-off in gains.  Often with PCA we can get \"the most bang for our buck\" with a few components that account for most of explained variance.  Looking at the plot above, what points seems like good choices? (this particular case may not have a great \"elbow\", so you might want to just pick an arbitrary cutoff).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your answer here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA plots\n",
    "\n",
    "Regardless of the best number of PCA components to use, we want to plot our data in 2d, in order to do this, we will just select the first two components.  How much variance do we keep with the first two dimensions? How will it inform your opinion of our 2d scatter plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the scaled data with PCA\n",
    "x_sca_pca = pca.transform(x_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a scatter plot of the PCA data\n",
    "# you'll want to use the first and second PCA components to plot.\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok! So the graph looks obviously quite promising. This also seems to line up with what our clusters told us.  Let's see if the cluster assignments fit the PCA plot correctly or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get labels from K-means\n",
    "# note we should predict on the same data we fit K-means with. So if we fit with data \"x_scaled\" we need to be sure to predict on that data.\n",
    "# it would be a mistake to make predictions on the PCA transformed data now, because we did not fit the kmeans object on the PCA data!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,6));\n",
    "\n",
    "# plot your datapoints again, this time with colors for labels.  I gave you some helper code below\n",
    "ax.scatter(x_sca_pca[:,0],x_sca_pca[:,1], c=labels, cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What do you think of the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Components examined\n",
    "\n",
    "Ok so we have plotted our clusters in 2D and it seems to be lining up with our previous analysis.  The next step is to examine the \"loadings\" which represent how much of the original features are contributing to our new PCA functions.\n",
    "PCA is a mathematical operation (SVD - singular value decomposition) a transformation from the original feature space into a new feature space. That transformation can be represented as a matrix - we call it the _covariance_ matrix and it's used to project the features from the original space into the PCA space. By examining the coefficients (loadings) in the covariance matrix we can get an idea what the new PCA features are composed of. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca_comps(pca_item, num_dims, data):\n",
    "    \n",
    "    variance_ratios = pca_item.explained_variance_ratio_\n",
    "    components = pd.DataFrame(pca_item.components_[:num_dims], columns = data.columns)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize = (14,8))\n",
    "    \n",
    "    components.plot(ax=ax, kind = 'bar');\n",
    "    ax.set_ylabel(\"Feature Weights\")\n",
    "    dims = [f'Dimension {i}' for i in range(num_dims)]\n",
    "    ax.set_xticklabels(labels =dims,rotation=0)\n",
    "    \n",
    "    # Display the explained variance ratios\n",
    "    for i, ev in enumerate(pca_item.explained_variance_ratio_[:num_dims]):\n",
    "        ax.text(i-0.40, ax.get_ylim()[1] + 0.05, f\"Explained Variance\\n {np.round(ev,4)}\")\n",
    "\n",
    "    ax.legend(loc='best', bbox_to_anchor=(0.75, 0.1, 0.5, 0.5)) # bbox is (x,y, width, height)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass PCA, # of dimensions to plot, X the data )\n",
    "plot_pca_comps(pca, 2, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What do the loadings tell us?\n",
    "\n",
    "The components of PCA each are created by combining all the existing features. We see the existing features in the legend and the PCA bar-plot tells us the coefficients of the covariance matrix found by PCA. When you are reading the bar plot, you should look for the lines with largest magnitude. The sign (positive vs negative) does not matter, because it can be reversed (it's random each time PCA runs, it can be either direction). The features which are contributing to that PCA dimension are the features with the largest magnitude. If they are in opposite directions it means the features have an anti-correlation to one another. If they are in the same direction, then the features are correlated and PCA is finding that.\n",
    "\n",
    "What can you tell from looking at PC1 and PC2 (the first two dimensions?).  Note that we were able to cluster very efficiently with just these two PC dimensions, which of these original features are defining these clusters?\n",
    "\n",
    "What relationship do you see between the centroids of your kmeans clusters and the PCA components?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feel free to look at more PCA loadings, extend the graph to all 6 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering on PCA transformed data\n",
    "Ok, our final idea!\n",
    "\n",
    " * Cluster our data upon the PCA tranformed data and see if we get similar results\n",
    " * To do this we need to first:\n",
    "  1. scale the data\n",
    "  2. fit_transform with pca\n",
    "  3. cluster on the PCA data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## do the work here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plot your results here\n",
    "fig, ax = plt.subplots(figsize=(10,6));\n",
    "ax.scatter(pca_scaled_data[:,0],pca_scaled_data[:,1], c=labels, cmap='viridis');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to look at the centroids we need to inverse them back into the scaled space.\n",
    "centers =   #your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_centroids(centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we learn clusters on the PCA data effectively? At least in this case\n",
    "\n",
    "Your answer here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mybase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
