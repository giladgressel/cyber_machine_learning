{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "\n",
    "#preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from scipy.stats import mode\n",
    "\n",
    "\n",
    "# pipelines\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Data\n",
    "\n",
    "In this assignment we are going to examine some customer data gathering from my very famous internet company \"the best one ever\".  \"The best one ever\" is the best company ever that sells important things online.  In this investigation we want to find if there are any natural groups of customers in my dataset.  The first step is to just get the data in a format we can feed to our machine learning models. Once we do that, then our boss (some dude named Gilad), said he will teach us how to cluster the customers! But it turns out you need to have the _data_ formatted in some special way...? Maybe you can tell me about that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data found in \"best_one_ever_database.csv\" and take a look at the head and info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning The Data\n",
    "# Cleaning The Data\n",
    "\n",
    "Taking a look at the data you have to ask yourself the questions\n",
    "\n",
    "1. 'Which columns are useful for me to keep?'\n",
    "2. 'Are all the columns usable as features?\n",
    "\n",
    "Then you may have to do some \"work\" to get the column to be usable. Let's look at one column together. The first column is titled \"first_name\" and it seems to be the first name of each customer. Is this a usable feature? Well... not exactly in string format. So I guess I could one hot encode them into binary vectors, but even then... do I want to cluster the customers based on their first name? You can imagine some situation where clustering by name might be relevant (for example trying to guess what generation someone belonged to?) but in this case it seems like it's more of a unique identifier so it may be best to simply remove it. If every value in a column is unique (there are no duplications of the value) then we shouldn't use it as a feature because it will have a 1-1 mapping with the target variable which is not something we ever want. We want our model to learn and generalize from the features, not memorize that the name \"jane\" bought 5 cans of soda.\n",
    "\n",
    "Ok, that's the first column, we vote drop! Now you have to go through each and every column and ask yourself \"do I keep it? if yes, what extra work might I have to do?\" \n",
    "\n",
    "Let's walk through it\n",
    "  \n",
    "  1. first_name:  this is a unique identifier so we should remove it.\n",
    "  2. last_name: see above\n",
    "  3. email: this is unique to an extent.  BUT if we strip the name@ portion of the email and simply keep the domain name, it could possibly aid us. Perhaps certain kinds of customers use certain email services! Worth looking into\n",
    "  4. Gender: this is certainly relevant, but it's categorical data. We will need to one-hot-encode it.\n",
    "  5. ip_address: We can perhaps segment the ip's into fields and use them, there maybe overlaps or correlations among different fields. Or maybe you know more about IP addresses than I do and this is totally useless\n",
    "  6. sales: we certainly need this column, but we need to convert it a floating point type: remove the '$' and convert the dtype of the column\n",
    "  7. zip_code: I think we can just leave this as is.\n",
    "  8. prob-of_rebuy: I think we can just leave this as is.\n",
    "  9. Money_spent: seems fine to me!\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the name columns using pandas.drop()\n",
    "X = data.drop() # fill in the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# always reasonable to check `.info()` or .head() after an operation to make sure it worked\n",
    "# how you thought, at least until you are comfortable with the methods / functions\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform string columns into useful features\n",
    "\n",
    "1. email column\n",
    "2. sales column (remove $ sign)\n",
    "\n",
    "We will use the pandas `apply` function take a function that operates on a string and apply it to the entire column. I will do the first one, and you will do the next one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_dollar(x):\n",
    "    return x[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function to the column and assign it back to the column (it does not work inplace)\n",
    "X.sales = X.sales.apply(strip_dollar)\n",
    "\n",
    "# cast the column to a floating point type - this is very important, otherwise it will be\n",
    "# an object type column that we cannot do arithmetic on the column\n",
    "X.sales = X.sales.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn\n",
    "\n",
    "Now you need to\n",
    "1. write a function to strip the name portion of the email\n",
    "2. Apply it to the column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions to apply to the dataframe\n",
    "def strip_emails(x):\n",
    "    # your code here\n",
    "\n",
    "test_email = 'thisismymail@gmail.com'\n",
    "print(strip_emails(test_email))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function to the column and assign it back to the column (it does not work inplace)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is the email column going to be worth it?\n",
    "Let's take a look at this email column and decide if it could help us or not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many unique domains are there?\n",
    "counts = X.email.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on what you think, you can either keep the column or not. It never hurts to try using it and then drop it later, it's also not a big deal if you are sure it's going to be useless. You will learn to have intuition with these things over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the IP Address\n",
    "We now need to split up the IP address, we will use Pandas's built in str method for this.\n",
    "Again, I'm not even sure this is a good idea, but it's certainly fine practice to learn how to spit up strings in pandas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to use pandas str method here, pay close attention!\n",
    "\n",
    "X[['first_ip','second_ip','third_ip','fourth_ip']] = X.ip_address.str.split(pat=\".\", expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we cast the columns as floats, because we always need numbers for our models!\n",
    "X[['first_ip','second_ip','third_ip','fourth_ip']] = X[['first_ip','second_ip','third_ip','fourth_ip']].astype('float32')\n",
    "# we also drop the original column\n",
    "X.drop('ip_address', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Encoding\n",
    "\n",
    "Ok we are almost done, we just have to convert the gender column into something integer that we can use. We will use one-hot-encoding since gender is a categorical variable.\n",
    "\n",
    "Pandas has a `get_dummies()` function that will be very useful.\n",
    "I'm going to let you look it up and learn how to use it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the gender distribution, just because\n",
    "X.gender.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing Stage 2\n",
    "\n",
    "Ok, we are done with stage 1 - we have converted everything into numeric features and dropped all the unneccessary things. Please double check that! Make sure all features are numeric (a check with .info() should do the trick).\n",
    "\n",
    "However we do have missing values. Which two columns have missing values?\n",
    "How many values are missing?\n",
    "What should we do about those missing values?\n",
    "\n",
    "You can either impute (fill in) the missing values, or drop the rows which contain them. The choice is up to you!\n",
    "Either way, you should practice both methods. This way you can practice coding both solutions.\n",
    "\n",
    "Note:\n",
    "The `DataFrame.fillna()` method essentially assumes that you are using timeseries data. We are not, so I wouldn't use this. In order to impute simple values, you can use numpy easily, but... I'm lazy and would probably use the scikit-learn implementation.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/impute.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
