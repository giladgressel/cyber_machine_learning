{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "\n",
    "# classifier we will use\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# model selection bits\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n",
    "from sklearn.model_selection import learning_curve, validation_curve\n",
    "\n",
    "# evaluation\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# plotting\n",
    "from plotting import plot_learning_curve, plot_validation_curve\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Spam with Decision Trees\n",
    "\n",
    "In the last assignment we worked with the spam database. We are going to do the same thing in this assignment and we will practice with the correct tools this time around. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we are going to hardcode the column names, because this just makes it a little easier to use pandas.\n",
    "\n",
    "names = ['word_freq_make:        ',\n",
    "'word_freq_address:     ',\n",
    "'word_freq_all:         ',\n",
    "'word_freq_3d:          ',\n",
    "'word_freq_our:         ',\n",
    "'word_freq_over:        ',\n",
    "'word_freq_remove:      ',\n",
    "'word_freq_internet:    ',\n",
    "'word_freq_order:       ',\n",
    "'word_freq_mail:        ',\n",
    "'word_freq_receive:     ',\n",
    "'word_freq_will:        ',\n",
    "'word_freq_people:      ',\n",
    "'word_freq_report:      ',\n",
    "'word_freq_addresses:   ',\n",
    "'word_freq_free:        ',\n",
    "'word_freq_business:    ',\n",
    "'word_freq_email:       ',\n",
    "'word_freq_you:         ',\n",
    "'word_freq_credit:      ',\n",
    "'word_freq_your:        ',\n",
    "'word_freq_font:        ',\n",
    "'word_freq_000:         ',\n",
    "'word_freq_money:       ',\n",
    "'word_freq_hp:          ',\n",
    "'word_freq_hpl:         ',\n",
    "'word_freq_george:      ',\n",
    "'word_freq_650:         ',\n",
    "'word_freq_lab:         ',\n",
    "'word_freq_labs:        ',\n",
    "'word_freq_telnet:      ',\n",
    "'word_freq_857:         ',\n",
    "'word_freq_data:        ',\n",
    "'word_freq_415:         ',\n",
    "'word_freq_85:          ',\n",
    "'word_freq_technology:  ',\n",
    "'word_freq_1999:        ',\n",
    "'word_freq_parts:       ',\n",
    "'word_freq_pm:          ',\n",
    "'word_freq_direct:      ',\n",
    "'word_freq_cs:          ',\n",
    "'word_freq_meeting:     ',\n",
    "'word_freq_original:    ',\n",
    "'word_freq_project:     ',\n",
    "'word_freq_re:          ',\n",
    "'word_freq_edu:         ',\n",
    "'word_freq_table:       ',\n",
    "'word_freq_conference:  ',\n",
    "'char_freq_;:           ',\n",
    "'char_freq_(:           ',\n",
    "'char_freq_[:           ',\n",
    "'char_freq_!:           ',\n",
    "'char_freq_$:           ',\n",
    "'char_freq_#:           ',\n",
    "'capital_run_length_average',\n",
    "'capital_run_length_longest',\n",
    "'capital_run_length_total: ',\n",
    "'label']\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load in the dataset here \n",
    "\n",
    "data = pd.read_csv('spambase/spambase.csv', names = names)\n",
    "X = data.drop('label', axis = 1)\n",
    "y = data.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this time we will keep a bit more data for training and validation.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.8, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Evaluate your classifier using 5 fold cross validation.\n",
    "\n",
    "Do you see any variance in different runs?\n",
    "Why do you think this is happening?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a default decision tree model\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# create a stratifiedKfold cv object -- we want stratified to keep the class balance across folds\n",
    "cv = StratifiedKFold(n_splits = 5)  # set it for 5 folds\n",
    "\n",
    "# get the scores back using the helper function, make sure to pass a scoring function string - otherwise you default to accuracy\n",
    "scores = cross_val_score( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking for variance in the runs\n",
    "\n",
    "How much do the runs change over time? Let's run a for-loop and check.  In fact, that's exactly what shuffle split will do for us, so we can just run 100 iterations with 25% held out as validations - this will mimick 5-fold CV in the long run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss_split = StratifiedShuffleSplit(n_splits = , test_size=) # fill in the params\n",
    "scores_sss = # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print at your own risk!\n",
    "print (scores_sss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_sss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting a histogram of the runs\n",
    "\n",
    "So we want to look for variance or variation in the runs. It's not easy to just look at a large list of scores and get any idea, so I will plot a histogram which should show us the distribution of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.displot(scores_sss,  aspect=2, kde=True, bins=15);  # go ahead and try different bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does the histogram tell you?\n",
    "\n",
    "What is the mean of the our model? What kind of variation can we expect?\n",
    "\n",
    "#### Your answer below : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tune your models hyper-parameters with Gridsearch\n",
    "\n",
    "Choose at least **3** parameters with **3** values each (total **9**) to search over\n",
    "When you create your GridSearchCV object make sure to set the following\n",
    "\n",
    "1. cv object - which CV do you want to use? This will be run for every experiment, so if you do 100 sss, then you get 100 experiments for every parameter setting! This will add up fast. So make sure to choose something reasonable.\n",
    "2. scoring parameter - never forget this!\n",
    "\n",
    "If you don't set the above, GridSearchCV will default to kfold=5 for the CV and accuracy for the scoring parameter.  kfold=5 isn't the worst (stratifiedkfold is better for us), but accuracy is really wrong for this dataset since it's unbalanced and a spam dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you need to setup a paramgrid.  I will give you one example\n",
    "\n",
    "param_grid = {'max_depth':[2,10,20,30]} # will check max depth for those four parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make a gridsearch object and pass all the needed parameters.\n",
    "grid = GridSearchCV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit your grid-search! - it's an estimator so you fit it like a normal model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1: Gridsearch Results \n",
    "\n",
    "1. What are the final parameters that were best?  (grid.best_params_)\n",
    "2. What was your models best score? (grid.best_score_)\n",
    "3. You can also print out the best estimator with grid.best_estimator_\n",
    "4. Go ahead and put your grid results into a dataframe. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(grid.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  Randomized Search\n",
    "\n",
    "#### Step 1, Distributions\n",
    "In order to do randomized search we need to draw from a distribution.  This can be a bit confusing so I will first just plot some distributions for you, then it will be pretty clear which ones we want to draw from.\n",
    "\n",
    "For decision trees most of the parameters we want to try out are integers. `min_samples_split`, `max_depth`, etc they are all integer numbers. So I will focus on distributions which draw integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(scipy.stats.norm(5, scale=2).rvs(1000),  aspect=2, kde=True,);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the gaussian or normal distribution.  In fact, we can't use it because it doesn't draw integers-- but I just wanted to show it to you! Well I suppose we could use it and then convert the output to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = scipy.stats.norm(5, scale=2).rvs(1000)\n",
    "sns.displot(numbers.astype(int),  aspect=2, kde=True,); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can use the gaussian and convert to integers, or we can draw from a uniform random distribution. That's plotted below. It basically says \"pick a random number! Any random number!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(scipy.stats.randint(0,50).rvs(1000),  aspect=2, kde=True);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we need to choose a distribution for our grid-search. Based on what area we want to search we can choose uniform or int-converted gaussian. If we choose guassian we can \"target\" it by adjusting the mean and std, similarly we can set the range on the uniform distribution (the low and high ends).  You can decide what you want for your distribution. I will give you on example of how to setup the paramter grid for randomized search below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_params = { 'max_depth': scipy.stats.randint(2,100),}\n",
    "\n",
    "## note that we don't say \"how many\" samples to draw in the param grid.\n",
    "## We will do that below when we create the RandomizedSearchCV object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we don't say \"how many\" samples to draw in the param grid. We will do that below when we create the RandomizedSearchCV object. You need to set the following\n",
    "* estimator\n",
    "* params\n",
    "* n_iters (this decides how many times to sample from the distribution per setting)\n",
    "* scoring\n",
    "* cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_search = RandomizedSearchCV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Did you find different best parameters?\n",
    "Which method do you like more?\n",
    "What method do you think got better results?\n",
    "Why? Why not?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You answers here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Validate your classifier using\n",
    "\n",
    "1. Learning Curve\n",
    "2. Validation Curves (complexity graph)\n",
    "\n",
    "Please plot at least 3 different validation curves for 3 different hyper-parameters than you can tune.\n",
    "\n",
    "You can (and should) use the imported functions `plot_validation_curve` and `plot_learning_curve`. These functions are originally found on sklearn's site but I did modify them slightly for our use. You can check out the full code in plotting.py, which also contains links to the original source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Choose your best parameters:\n",
    "\n",
    "Keep in mind that the random-search and grid-search will always maximize the validation score, even if it's in the area of overfitting. Looking at the validation curves you created, what you do think are the best parameter values?\n",
    "\n",
    "Now create a tree model with this and fit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Looking at Feature Importances\n",
    "\n",
    "Decision trees have a nice property that they can automatically sort your features by importance. This just naturally falls out from the gini coefficient or the information gain ratio (whichever one you used).  Therefore we can run a little code and actually see which features our model found to be the most important.\n",
    "\n",
    "\n",
    "The code from Chris Albon below will help you do this, go ahead and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://chrisalbon.com/machine_learning/trees_and_forests/feature_importance/\n",
    "\n",
    "importances = my_fav_clf.feature_importances_\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# rearrange feature names so they match the sorted feature names\n",
    "# I have my original names list of feature names, which corresponds to the features in the tree.\n",
    "f_names = [names[i] for i in indices]\n",
    "\n",
    "#create my plot\n",
    "fig, axes = plt.subplots(figsize=(20,5))\n",
    "axes.set_title(\"Feature Importances\")\n",
    "axes.bar(range(X_train.shape[1]), importances[indices])\n",
    "\n",
    "# Add feature names as x-axis labels\n",
    "axes.set_xticks(range(X.shape[1]));\n",
    "axes.set_xticklabels(f_names, rotation=45, ha='right');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the most important feature that your model found? Does this surprise you, why or why not?\n",
    "\n",
    "#### Your Answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test your data.\n",
    "\n",
    "Retrain your model using all the training data and using the hyper parameters you think are best. \n",
    "Then go ahead and test your model with the test data. Evaluate your model by testing on the test set. How did you do? What is your performance?\n",
    "\n",
    "As always, ask yourself the question \"If I was doing this professionally for a company, what other questions would I want answered?\" Is this model ready to go??\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
