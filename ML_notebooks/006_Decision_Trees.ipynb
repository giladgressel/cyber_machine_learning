{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# classifier we will use\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# model selection bits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# evaluation\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Spam with Decision Trees\n",
    "\n",
    "In this assignment you will detect spam with decision trees. It's always important to investigate the data that you are using for any project. Since data is our gold mine, it's our oil that powers our models we need to have good quality data. Machine learning follows the \"garbage in, garbage out\" principle, if we feed in bad data for training, we will get a model that produces bad results.  For these reasons we want to answer the following questions.\n",
    "\n",
    "## 0.  Learn about the data\n",
    "\n",
    "1. Where did this data come from?\n",
    "2. Who made it?\n",
    "3. How were the features selected?\n",
    "4. Can you trust it?\n",
    "\n",
    "Go ahead and scan through the spambase_features.txt file and the spambase.txt file.  These two files provide information about the dataset, how it was curated and where it came from. Then try to answer the above questions.\n",
    "\n",
    "\n",
    "## 1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we are going to hardcode the column names, because this just makes it a little easier to use pandas.\n",
    "\n",
    "names = ['word_freq_make:        ',\n",
    "'word_freq_address:     ',\n",
    "'word_freq_all:         ',\n",
    "'word_freq_3d:          ',\n",
    "'word_freq_our:         ',\n",
    "'word_freq_over:        ',\n",
    "'word_freq_remove:      ',\n",
    "'word_freq_internet:    ',\n",
    "'word_freq_order:       ',\n",
    "'word_freq_mail:        ',\n",
    "'word_freq_receive:     ',\n",
    "'word_freq_will:        ',\n",
    "'word_freq_people:      ',\n",
    "'word_freq_report:      ',\n",
    "'word_freq_addresses:   ',\n",
    "'word_freq_free:        ',\n",
    "'word_freq_business:    ',\n",
    "'word_freq_email:       ',\n",
    "'word_freq_you:         ',\n",
    "'word_freq_credit:      ',\n",
    "'word_freq_your:        ',\n",
    "'word_freq_font:        ',\n",
    "'word_freq_000:         ',\n",
    "'word_freq_money:       ',\n",
    "'word_freq_hp:          ',\n",
    "'word_freq_hpl:         ',\n",
    "'word_freq_george:      ',\n",
    "'word_freq_650:         ',\n",
    "'word_freq_lab:         ',\n",
    "'word_freq_labs:        ',\n",
    "'word_freq_telnet:      ',\n",
    "'word_freq_857:         ',\n",
    "'word_freq_data:        ',\n",
    "'word_freq_415:         ',\n",
    "'word_freq_85:          ',\n",
    "'word_freq_technology:  ',\n",
    "'word_freq_1999:        ',\n",
    "'word_freq_parts:       ',\n",
    "'word_freq_pm:          ',\n",
    "'word_freq_direct:      ',\n",
    "'word_freq_cs:          ',\n",
    "'word_freq_meeting:     ',\n",
    "'word_freq_original:    ',\n",
    "'word_freq_project:     ',\n",
    "'word_freq_re:          ',\n",
    "'word_freq_edu:         ',\n",
    "'word_freq_table:       ',\n",
    "'word_freq_conference:  ',\n",
    "'char_freq_;:           ',\n",
    "'char_freq_(:           ',\n",
    "'char_freq_[:           ',\n",
    "'char_freq_!:           ',\n",
    "'char_freq_$:           ',\n",
    "'char_freq_#:           ',\n",
    "'capital_run_length_average',\n",
    "'capital_run_length_longest',\n",
    "'capital_run_length_total: ',\n",
    "'label']\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load in the dataset here \n",
    "data_location = Path.cwd()/'spambase'/'spambase.csv'\n",
    "data = pd.read_csv(data_location, names = names)\n",
    "X = data.drop('label', axis = 1)\n",
    "y = data.label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess the data if needed\n",
    "\n",
    "1. Are there any empty values?\n",
    "2. Do you need to transform the data?\n",
    "3. What is the distribution of the positive and negative classes?\n",
    "4. Split the data into training and testing sets\n",
    "\n",
    "Let me give a few hints.  When it comes to scaling the data, we normally just _should_, but in this case we are going to be working with decision trees and I think we learned that they have an interesting property!  \n",
    "\n",
    "We want to look at the distribution of positive (spam) and negative (ham) classes.  So basically we need to look at the count of the labels.  You can use the function `.value_counts()` on `y`.  Note that the function `value_counts` is specific to the Series class, it doesn't work on Dataframes.  Now it's not just enough to look at the raw numbers, I suggest you calculate statistics like \"what percentage of my data is ham? What percentage is spam?  This may help you decide if you should use the `stratify` keyword when splitting your data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split your data with 70% for training, this is somewhat random, but this is how I will do it in the \n",
    "## solution video so you will have similar results. Feel free to change this value and experiment if you like.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train a Decision Tree Classifier\n",
    "\n",
    "1. What metric should we use? The f1-score or accuracy?\n",
    "\n",
    "Let's start with a default model, we won't specify any settings on the decision tree model for the first training. The following will be a 3-step process\n",
    " * training\n",
    " * getting predictions\n",
    " * evaluating our predictions\n",
    " \n",
    "The question does arise, what should we check our predictions on? The obvious answer is that we should check the predictions on our testing set. That's the set of data that simulates our future unseen data. However in our quest to figure out if we are overfitting or not, it could be very useful to look at the performance of the training set.  Remember when we overfit our polynomials and they touched every point? But then we added data the performance would drop. Similarly if the perfomance of the training data is near perfect, but the testing data is much worse, that _gap_ indicates overfitting. So we will check both training and testing performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get predictions from the model on the training data and the testing data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## evaluate the predictions\n",
    "## Note that the order of the arguments is **very** important for f1-score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Did we overfit? Either way, let's try out some of the decision tree parameters\n",
    "\n",
    "Go ahead and try out some different tree parameters. \n",
    "\n",
    "* `max_depth`\n",
    "* `min_samples_split`\n",
    "* `min_samples_leaf`\n",
    "\n",
    "All three of these parameters will control the complexity of the tree.  Before you try them out, let's take a quick quiz:\n",
    "\n",
    "* increasing `max_depth` will : **Increase** or **Decrease** overfitting?\n",
    "* increasing `min_samples_split` will: **Increase** or **Decrease** overfitting?\n",
    "* increasing `min_samples_leaf` will: **Increase** or **Decrease** overfitting?\n",
    "\n",
    "It's very important to know the answer to these questions - because other wise you can't tune the parameters correctly. If you aren't sure go back and check the quiz on this very topic.\n",
    "\n",
    "Go ahead and try at least 3 values for each parameter. Then if you try 3 values for each combination, you actually would try 9 total parameters. You can see where this is going -- `for` loops! At this point I encourage you to just try whatever method you want, you can ad-hoc try different values or you can get robust with for-loops. Whatever you are inspired to do. The main point is to try out some values and see what you find.  What helps improve our testing performance the best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ok, now let's do it systematically\n",
    "\n",
    "Let's do it with some for loops. This will lead us to having more data than we can \"look\" at, so naturally we will plot it.\n",
    "I've started the for loop for you, you need to fill in the inner part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = []\n",
    "test_results = []\n",
    "for i in range(2,50): # feel free to change these, I just threw out a reasonable option here.\n",
    "    \n",
    "    dtc = DecisionTreeClassifier(=i)  #select a parameter to check\n",
    "                                      # train the model\n",
    "                                      # get predictions for both training and test\n",
    "                                     # evaluate them and append them into the our lists.\n",
    "\n",
    "print(train_results)\n",
    "print ( \" \")\n",
    "print (test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That's a lot of numbers!\n",
    "\n",
    "What do all those numbers tell us? Well we can pick out the best values using something like `np.max` and then we can relate it to the best parameter with the argument that correlates to that value (so depth 2 gave us .85 score etc).  But I think we'll find more interesting to plot these points.  I've provided the code for you below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize = (10,6));\n",
    "ax.plot(train_results, color = 'r', label = 'train')\n",
    "ax.plot(test_results, color = 'b', label = 'test')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This graph is called a validation curve\n",
    "\n",
    "What we have done is plotted the performance of the training and testing data against a parameter that was varied. How do we read this graph?\n",
    "What information can you glean?\n",
    "When does our model start to overfit?  \n",
    "Based on what you see here, what do you think the best choice for the parameter you were checking is?  That is, looking at the graph, what value would you choose for a spam detection model that you were going to deploy? Don't forget the purpose of the model!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answers here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally... we ... made a mistake!\n",
    "Do you see what the giant mistake we did? Go ahead and let us know below.\n",
    "If you don't see the mistake... don't worry I'll explain in the next unit!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e4f6f65502d94df457f6ff4504d6f624c18b93116c34597ccd9cc57c3f004009"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
