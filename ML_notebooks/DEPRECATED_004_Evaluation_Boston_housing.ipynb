{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyRegressor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boston Housing with Linear Regression\n",
    "\n",
    "In this notebook we will work with the classic [Boston Housing dataset](https://scikit-learn.org/stable/datasets/index.html#boston-house-prices-dataset) again!\n",
    "\n",
    "This time we will fix up some of our previous mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      ".. topic:: References\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.data\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(X, columns=data.feature_names) # little trick to get the column names in correctly\n",
    "y = pd.Series(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split\n",
    "\n",
    "Ok, now we're going to divide up our dataset so that we have an unseen test set that we can evaluate against. This way we can **simulate** tomorrows world and figure out if our trained model can generalize to unseen data at all.\n",
    "\n",
    "Here is the code to split up our data. It's important to remember how the arrays are passed back, it's always in this exact order, so don't forget!! but if you do forget... you can always look it up [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) (scroll down for the code example).  I have looked it up probably 100 times until I remembered it while teaching this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## you need to set the test size!\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a look at how many samples we got, just to check.\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling our dataset\n",
    "\n",
    "Ok, we need to scale our dataset.  We learned about two choices so you can pick from two options!\n",
    "\n",
    "* StandardScaler\n",
    "* MinMaxScaler\n",
    "\n",
    "Now there is a small wrinkle.  We need to fit and transform on our `X_train` data, but we __only__ `transform` the test data.  This is because the transformation requires __knowing__ something about the dataset.  We actually have our scaler learn that from the data with `fit`, but since we are simluating tomorrows world, we absolutely do not know the mean and std of the test data! Therefore we will scale the test data with whatever stats we learned from the training data and hope it's close enough.  This is the messy reality of machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialize a scaler here\n",
    "\n",
    "scaler = # your choice of scaler here (check the imports to remember their names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## first we need to `.fit()` our scaler on the dataset\n",
    "## this teaches the scaler the mean values of the features and the standard deviation.\n",
    "## these two values are used in the transformation process.\n",
    "## You should only call `.fit` on training data! Never on the testing data!\n",
    "scaler.fit(X_train)\n",
    "# now we transform the data\n",
    "X_train_scaled = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we need to transform our testing set here\n",
    "\n",
    "X_test_scaled = #use the scaler transform function on our dataset to return a new dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a model\n",
    "\n",
    "Ok it's time to train our model.  We will want to choose one of the three options we imported earlier. The only difference here is that we will train __only__ on the training data.\n",
    "\n",
    "* LinearRegression\n",
    "* SGDRegressor\n",
    "* Ridge\n",
    "\n",
    "We are also going to train a [dummy regressor](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyRegressor.html).  This regressor will use a default strategy like 'mean' or 'median'. That means it will always guess the average value in the dataset, which is pretty dumb,b ut we need to prove to ourselves that our model learned something -- so this is a good way to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a model here\n",
    "\n",
    "reg = # initialize your model here\n",
    "reg_dummy = DummyRegressor(strategy='') # put a strategy into the keyword argument there!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your model here with .fit\n",
    "\n",
    "reg.fit(, ) # only use the training data for fitting\n",
    "reg_dummy.fit(,) # we gotta fit our dummy too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate our model\n",
    "\n",
    "Ok, let's use our trained model to make predictions, then we can evaluate those predictions against the real known `y` values. What should we evaluate against though?  Should we evaluate the training data? Or just the test data?\n",
    "You can decide, but obviously we should definitely evaluate the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use your model to make predictions on the data\n",
    "\n",
    "y_pred = # your model.predict here  -- now you can use X_test\n",
    "dumb_y_preds = reg_dummy.predict()  # put in the testing x to make some predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok now we need to evaluate our predictions. We will use scikit-learns inbuilt mean squared error metric for this. It's very important that you pass your arguments correctly to the evaluation function, all scikit-learn metrics use `y_true, y_pred` format, which means pass the ground-truth first, followed by the prediction.\n",
    "\n",
    "Ok let's compare our results to our dummy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_score = mean_squared_error(y_test, y_pred)\n",
    "dummy_score = mean_squared_error(y_test, dumb_y_preds)\n",
    "print (f\"the MSE for our regressor is:{reg_score}\")\n",
    "print (f\"the MSE for our dummy is:{dummy_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That's a wrap!\n",
    "\n",
    "Any issues? Are we happy with everything?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e4f6f65502d94df457f6ff4504d6f624c18b93116c34597ccd9cc57c3f004009"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
