{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning to use NLP features and Pipelines\n",
    "\n",
    "Especially pipelines!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler\n",
    "\n",
    "# pipelines\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# NLP transformers\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "# classifiers you can use\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# model selection bits\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score, ShuffleSplit\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold, KFold\n",
    "\n",
    "# evaluation\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "# dataset\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# plotting\n",
    "from plotting import plot_learning_curve, plot_validation_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data\n",
    "\n",
    "We are going to be using the 20 News Groups dataset available from [here](http://qwone.com/~jason/20Newsgroups/).  The data is a bunch of news articles, grouped into 20 news groups!\n",
    "\n",
    "Our goal is to classify the documents, into the category it belongs to.  The skills we learn in this assignment will be the following\n",
    "\n",
    "* basics of NLP (tf-idf, n-grams)\n",
    "* how to use pipelines\n",
    "* practice with the scikit-learn dummy model\n",
    "\n",
    "Note that when we load the data we are going to exclude the follow columnts: \"headers\", \"footers\", and \"quotes\" as these are meta-data columns which provide extra information about the news articles.  These features are manually added, so we are removing them since we'd like to try and learn what class an article belongs to using the information present in the article alone.  \n",
    "\n",
    "**NOTE**\n",
    "Due to the way this data is stored, it's actually easier to leave it in it's native format (list and array) from sci-kit learn. So we won't be bothering with pandas here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_20newsgroups(categories = [ 'comp.graphics',\n",
    " 'comp.os.ms-windows.misc',\n",
    " 'comp.sys.ibm.pc.hardware',\n",
    " 'talk.politics.guns',\n",
    " 'talk.politics.mideast',\n",
    " 'talk.religion.misc'],remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 is comp.graphics\n",
      "1 is comp.os.ms-windows.misc\n",
      "2 is comp.sys.ibm.pc.hardware\n",
      "3 is talk.politics.guns\n",
      "4 is talk.politics.mideast\n",
      "5 is talk.religion.misc\n"
     ]
    }
   ],
   "source": [
    "for i,name in enumerate(data.target_names):\n",
    "    print (f\"{i} is {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have only loaded in two topic types \"computer\" and \"talk\". Furether, I selected only 6 totaltopics from the 20 total that the dataset has to offer. Further we are going to simplify the problem even further, and lump together the two topics into two general topics.  Why?  Because otherwise it takes quite a while to run a single model on this dataset.  After you have built a decent model and tuned it, then you can try loading in more data and attempting to do multi-class classification.  For now we will focus on a simpler binary classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ONLY RUN THIS CELL ONCE ###\n",
    "## RUNNING IT MULTIPLE TIMES WILL BREAK EVERYTHING ###\n",
    "\n",
    "# set the 3 computer targets to one label\n",
    "data.target[data.target == 1] = 0\n",
    "data.target[data.target == 2] = 0\n",
    "\n",
    "# set 3 talk targets to one label\n",
    "data.target[data.target == 3] = 1\n",
    "data.target[data.target == 4] = 1\n",
    "data.target[data.target == 5] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.target_names = [\"computer\",'talk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking that I only have two classes left!\n",
    "np.unique(data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1765, 1487])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's check the distribution of my two classes\n",
    "np.bincount(data.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print a single sample from the news groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I do not think it is at all unlikely that Clinton ro his policy\\nwonk facilitators arranged the Waco raid as a display piece for  the\\nGun War on the Constitution.  Look at what the Bush administration did to\\nget material for the Drug War on the Constitution--remember that baggie of\\ncrack George waved at the cameras?  They took a dealer from the ghetto\\nand brought him to the White House so they could say drugs had been\\ndealt onb the White House Lawn.\\nAnd I don't think anybody could honestly think Clinton would have any\\nmoral qualms about the raid...\\nThe only really worrisome thing is that the BD's heroic defense of\\ntheir ranch will make Clinton's Gun War on the Constitution _more_\\nsuccessfull--exactly as he wanted.  The media and politicians will\\nfilter this so that the general public will think the BD's\\nare bad guys!  Don't help them.  Stand up for the BD's with your\\nfriends and family adnd in public anytime you can--their supposed\\nmoral qualms are not important to the issue.  They are heroes in the\\nfight against oppressive government;  it could just as well have been\\nyou.\\n-watkins@earth.eecs.uic.edu  (Brian E Watkins)\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data.data\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are a bunch of weird characters in there, for example I see a bunch of `\\n` character patterns. This is a newline character.  I wonder if we will need to deal with this in any special way when we do our NLP feature extraction?\n",
    "\n",
    "Let's try printing `X[0]` and see if it's different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I do not think it is at all unlikely that Clinton ro his policy\n",
      "wonk facilitators arranged the Waco raid as a display piece for  the\n",
      "Gun War on the Constitution.  Look at what the Bush administration did to\n",
      "get material for the Drug War on the Constitution--remember that baggie of\n",
      "crack George waved at the cameras?  They took a dealer from the ghetto\n",
      "and brought him to the White House so they could say drugs had been\n",
      "dealt onb the White House Lawn.\n",
      "And I don't think anybody could honestly think Clinton would have any\n",
      "moral qualms about the raid...\n",
      "The only really worrisome thing is that the BD's heroic defense of\n",
      "their ranch will make Clinton's Gun War on the Constitution _more_\n",
      "successfull--exactly as he wanted.  The media and politicians will\n",
      "filter this so that the general public will think the BD's\n",
      "are bad guys!  Don't help them.  Stand up for the BD's with your\n",
      "friends and family adnd in public anytime you can--their supposed\n",
      "moral qualms are not important to the issue.  They are heroes in the\n",
      "fight against oppressive government;  it could just as well have been\n",
      "you.\n",
      "-watkins@earth.eecs.uic.edu  (Brian E Watkins)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup, those `\\n` must be newline characters. They print out fine.  Still I don't think we will want them in our classification model.. or maybe we do? We will have to figure that out.\n",
    "\n",
    "Let's also take a look at the target names -- the category labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['computer', 'talk']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What category do you think that first example belongs to?  Let's check it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'talk'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data.target\n",
    "data.target_names[y[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup, that seems like an accurate label! It's a \"Talk\" topic.  Ok... now let's build a model and classify these guys!\n",
    "Let's try to follow the basic steps outlined below. Some of them you have seen before, some are new.\n",
    "\n",
    "* split into training / test sets (20% would be a good test set size)\n",
    "* extract features from the training data\n",
    "  * we should use a pipeline for this if we want to do cross validation!\n",
    "* do cross validation on a basic model and see how it performs\n",
    "  * plot learning curves and validation curves on our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training / testing\n",
    "# note I'm setting a random_state, normally I wouldn't but... I want to be able to describe the data to you, and I need to be\n",
    "# sure you have the same split I get.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.2, random_state=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features. \n",
    "Ok let's create a CountVectorizer which will create n-grams for us.  You can make a default one if you want (n-grams will be = 1, which is unigrams) or you can specify what length n-grams you want.\n",
    "\n",
    "First we will *fit* the vectorizer. This just \"learns\" all the different n-grams and stores them in a dictionary\n",
    "\n",
    "## A toy example first\n",
    "\n",
    "Let's start with something so small we can get a feeling for it.  We can start with two sentences in a list and we will vectorize those!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toys = [\"this is the first sentence in this list\", \"this is the second sentence\"]\n",
    "test_count = CountVectorizer()\n",
    "test_count.fit(toys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, no we have fit our vectorizer.  Let's see what kind of things we can find with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 7,\n",
       " 'is': 2,\n",
       " 'the': 6,\n",
       " 'first': 0,\n",
       " 'sentence': 5,\n",
       " 'in': 1,\n",
       " 'list': 3,\n",
       " 'second': 4}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_count.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is a unique dictionary of words, each word is mapped to a single integer. It's not a count of the word, it's just an index for it. It's the vocabulary found by the vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x8 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 12 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toys_transformed = test_count.transform(toys)\n",
    "toys_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now we transformed our \"toys\" into a a vectorized format.  It's important that we realize the first step was \"fitting\" the vectorizer, which means it was breaking down the words, finding all the unique unigrams (or whatever amount of grams we asked for) and then putting them into a dictionary.  The next step is to actually _transform_ our dataset into a vector.  What we got back was a sparse array!\n",
    "\n",
    "Sparse arrays are like regular arrays, but they contain mostly zeros so numpy just stores the indices of where the real values actually are.  Lets try to look into ours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 0, 1, 1, 2],\n",
       "       [0, 0, 1, 0, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can transform it into a full array, but you shouldn't really do that....\n",
    "toys_transformed.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 7)\t2\n",
      "  (1, 2)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 7)\t1\n"
     ]
    }
   ],
   "source": [
    "print(toys_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the two representations of the array are actually the same.  In the \"regular\" array we get `[1,1,0,1,1]` in the spare array it tells us that (0,0) = 1, (0,1) = 1, (0,3) = 1 ... etc.  That means (0,0) is \"the zeroth row and zeroth column is 1), which is what we see.  Note that the count_vectorizer is _counting_, if a word appeared twice, it's listed as 2. The sparse array only tells us about the _real_ non-zero values.  Scikit-learn is automatically creating a sparse array with the count-vectorizer because it's assuming that most of our arrays will be very sparse.  That is, most of our transformed data will be 0's!  Can you think about why this is true?\n",
    "\n",
    "Ok, let's run this on our actual data now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count vectorizer, easiest thing to do. Will make n-grams for us.\n",
    "count_vect  = CountVectorizer()\n",
    "count_vect.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50319"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got ~50,000 unigrams out of that! Wow.  Let's transfrom our data now and see how sparse it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vect = count_vect.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2601, 50319)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vect.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we have a shape of 2601 x 50319\n",
    "That doesn't tell us much about how much a single sample actually has in terms on non-zero elements.  We can check the non-zero elements in two easy ways\n",
    "\n",
    "  1. We can use `.getnnz()` which means \"get non zero\" -- however this tells us the number of non-zero elements, not their sum (not how many times they occured). \n",
    "  2. We can just sum a row with `.sum()` this will tell us how many total elements exist in the array, but we won't know how many are unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "print(X_train_vect[0].getnnz())  #getnnz --> \"get non zero\" will tell us how many non-zero entries we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "print(X_train_vect[0].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we see that for the first entry we have only 23 non-zero values in our count matrix.  This means it has 56500 empty values! Now you can see why we use a sparse matrix.  We are in 56,000 dimensions, but with only 23 points! The curse of dimensionality could really hurt us here...\n",
    "\n",
    "# Next step - train our baseline model\n",
    "\n",
    "Use Scikit-Learn's dummy models to make a baseline against. We need to reliably beat this monster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5428681654921567 is the mean score\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.54262673, 0.5432526 , 0.54272517])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy = DummyClassifier(strategy='most_frequent')\n",
    "scores = cross_val_score(dummy, X_train, y_train, cv=3, scoring = \"accuracy\") \n",
    "print (f\"{scores.mean()} is the mean score\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a real model - with pipelines\n",
    "Now, up until this point we have actually created enough \"features\" that we can train a model. If we run any kind of kfold validation, it would _work_. But it would be very wrong to do that with our current setup.  The thing is, when we perform k-fold validation, we want to simulate testing as if we had a real test set.  Currently we do _not_ have a real test, **because** we have transformed our entire X_train into a count vectorized format.  Let me try to explain the problem.\n",
    "\n",
    "The problem is that the dictionary is learned on the training data and what happens if we see a new word in the validation data, that didn't occur in the testing data? What feature value do we give that? Currently we wouldn't have to worry about that because _all_ samples in training have been converted into vectorized features.\n",
    "\n",
    "But we should have to worry about that!  What if tomorrow we get a new sample of news that contains words we have never seen before? What would we do?  We would need a strategy for dealing with _new never seen before words_. \n",
    "\n",
    "The default behavior of the count-vectorizer is to _ignore_ any new words it has never seen when asked to transform new data.  You can read this [here](https://scikit-learn.org/stable/modules/feature_extraction.html#common-vectorizer-usage).  Note, this is _only one way_ to handle unseen information. There are other strategies, but the point is _you need a strategy_.  \n",
    "\n",
    "If we just run cross-validation right now, with our vectorized X_train, we won't be accounting for this very real problem that _will_ occur with our never before seen testing data. **We need to simulate having words we have _never seen before_.** Therefore we actually need to create the count_vector dictionary and do the transformation for _every fold_ of the cross validation flow.\n",
    "\n",
    "On the first fold, we need to fit a count_vectorizer on 4/5's of the training data and transform it, and then use the same count_vectorizer to transform the held-out validation data.  If it finds words it's never seen before, it will just ignore them.  This will correctly simulate testing data.\n",
    "\n",
    "But how do we setup scikit-learn to automatically fit a transformer on every step of cross-validation!?\n",
    "PIPELINES!\n",
    "\n",
    "With scikit-learn we can setup a pipeline that will do a pre-determined set of steps every single time we call `fit` on it.  It's very easy. Watch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the mean score is: 0.8108346312728172\n",
      "The scores were: [0.82917466 0.79654511 0.79807692 0.83461538 0.79576108]\n"
     ]
    }
   ],
   "source": [
    "# create the pipeline\n",
    "dt_pipe = Pipeline([('vect', CountVectorizer(ngram_range=(1,1))),('clf', DecisionTreeClassifier())])\n",
    "\n",
    "# pass the pipeline as if it was the classifier into a cross validation method\n",
    "# the cv method will automatically figure out what to do.\n",
    "scores = cross_val_score(dt_pipe, X_train, y_train, cv=5, scoring = 'accuracy')\n",
    "print(f\"the mean score is: {scores.mean()}\")\n",
    "print(\"The scores were:\", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down what I just did.\n",
    "1. I created a pipeline\n",
    "2. I passed the pipeline as if it was the classifier, and everything just worked (during each fold of cross validation, it called `fit_transform` on training with CountVectorizer, followed by `fit` and `predict` with KNN).\n",
    "\n",
    "So how... how does the pipeline work?  Scikit-Learn is very clear about this [here](https://scikit-learn.org/stable/modules/compose.html#pipeline) and [here](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline), the gist is \"All estimators in a pipeline, except the last one, must be transformers (i.e. must have a transform method). The last estimator may be any type (transformer, classifier, etc.).\"\n",
    "\n",
    "CountVectorizer is a transformer, it transforms our data into another format.  We have seen other transformers already (normalizers and standard scalers).  Any object in Scikit-Learn that has a `.transform()` method is a transformer and valid for a pipeline.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting better features\n",
    "\n",
    "Ok, so CountVectorizer at it's basic is just making unigrams.  Can we do better?\n",
    "Well, we have two easy things we can try\n",
    "1. do more than unigrams by setting the option in `CountVectorizer(1,3)` will give us unigrams, bi-grams, and trigrams.\n",
    "2. Use a term-frequency inverse document frequency transformer (TF-IDF).\n",
    "\n",
    "TF-IDF will give a score to each n-gram weighted by the number of times it shows up in the document relevant to the number of times it shows up in the corpus (collection of documents).\n",
    "\n",
    "Let's try a few combinations with DT (which performed OK so far) and see if we can get it better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8077584492659993\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.83301344, 0.79270633, 0.80576923, 0.80961538, 0.79768786])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's make unigrams and bigrams\n",
    "count = CountVectorizer(ngram_range=(1,2))\n",
    "clf = DecisionTreeClassifier()\n",
    "dt_pipe = Pipeline([('vect', count),('clf', clf)])\n",
    "scores = cross_val_score(dt_pipe, X_train, y_train, cv=5, scoring = 'accuracy')\n",
    "print(scores.mean())\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That actually did worse (only marginally)\n",
    "It could very well be because we increased the dimensions even more by adding bigrams? Let's check our new dictionary length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "335645"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count.fit(X_train).vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, wow, bigrams blew us up from 50k features (dimensions) to 335,000.  That could have affected our scores.  Let's set `max_features` on our vectorizer, this will automatically \"only consider the top max_features ordered by term frequency across the corpus.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make unigrams and bigrams, we will limit to 65k features\n",
    "count = CountVectorizer(ngram_range=(1,2), max_features=65_000)  #yes you can use _'s in integers!\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "dt_pipe = Pipeline([('vect', count),('clf', clf)])\n",
    "scores = cross_val_score(dt_pipe, X_train, y_train, cv=5, scoring = 'accuracy')\n",
    "print(scores.mean())\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that didn't really seem to help much.  Let's try going back to unigrams, but adding a TF-IDF vectorizer. Note that the TF-IDF vectorizing takes as it's input a count_matrix, which is exactly what our count-vectorizer builds. So these two items are built to be used together (scikit-learn has a convenence function called tfidf_vectorizer that does both steps automatically)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make unigrams , we will limit to 10k features\n",
    "count = CountVectorizer(ngram_range=(1,1), max_features=10_000)  #yes you can use _'s in integers!\n",
    "clf = DecisionTreeClassifier()\n",
    "#tf-idf\n",
    "tfidf = TfidfTransformer()\n",
    "\n",
    "dt_pipe = Pipeline([('vect', count),('tfidf',tfidf),('clf', clf)])\n",
    "scores = cross_val_score(dt_pipe, X_train, y_train, cv=5, scoring = 'accuracy')\n",
    "print(scores.mean())\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = CountVectorizer(ngram_range=(1,3), max_features=10_000)  #yes you can use _'s in integers!\n",
    "clf = DecisionTreeClassifier()\n",
    "#tf-idf\n",
    "tfidf = TfidfTransformer()\n",
    "\n",
    "dt_pipe = Pipeline([('vect', count),('tfidf',tfidf),('clf', clf)])\n",
    "scores = cross_val_score(dt_pipe, X_train, y_train, cv=5, scoring = 'accuracy')\n",
    "print(scores.mean())\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, this was terrible, it's not getting any better. I have no idea!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your job.\n",
    "\n",
    "1. Create and try different decision trees, adaboost, and random forests. You can use other models if you want (scikit-learn models are easy to use!)\n",
    "2. Try adjusting the TFIDF parameters , n-gram parameters, and max_feature a parameter.\n",
    "3. Try different settings for your algorithms, max depth, number of estimators etc.\n",
    "4. Plot 1 Learning curve **and** 2 validation curves for your two most promising models.  Try to tune their parameters and get a higher accuracy. This means a total of 2 learning curves and 4 validation curves.\n",
    "\n",
    "Note that you can run gridsearch over the parameters in count-vecotrizer and tf-idf, it takes a bit of figuring out with the scikit learn documentation (how to pass the grid).\n",
    "When plotting your validation curves and using the pipeline you will need to pass the named step.\n",
    "Here is an example\n",
    "\n",
    "`pipe.named_steps` will printout all the names of my pipeline.\n",
    "Then once I know the name of the step in my pipeline (you set the names yourself when you create the pipeline), I can use the name and `__` to access it's parameters.\n",
    "For example if I named my classifier `clf` and it was a Decisiontree, if I wanted to adjust the `max_depth` on a validaiton curve I would run:\n",
    "\n",
    "`param_name='clf__max_depth'`\n",
    "This would access the `max_depth` attribute of my clf.\n",
    "\n",
    "Post your best models on the forums!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
