{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "\n",
    "#preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from scipy.stats import mode\n",
    "\n",
    "\n",
    "# pipelines\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Customer Data\n",
    "\n",
    "In this notebook you will use your previously cleaned and preprocessing customer dataset to practice clustering.\n",
    "I've implemented my own version below (of the preprocessing and cleaning) but you should feel totally free to use your own.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data found in \"best_one_ever_database.csv\" and take a look at the head and info.\n",
    "data = pd.read_csv(\"best_one_ever_database.csv\", index_col = 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning The Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the name columns using pandas.drop()\n",
    "\n",
    "X = data.drop(labels = ['first_name','last_name'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform string columns into useful features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_dollar(x):\n",
    "    return x[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function to the column and assign it back to the column (it does not work inplace)\n",
    "X.sales = X.sales.apply(strip_dollar)\n",
    "\n",
    "# cast the column to a floating point type - this is very important, otherwise it will be\n",
    "# an object type column that we cannot do arithmetic on the column\n",
    "X.sales = X.sales.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions to apply to the dataframe\n",
    "def strip_emails(x):\n",
    "    at = x.find('@')\n",
    "    return x[at+1:]\n",
    "\n",
    "test_email = 'thisismymail@gmail.com'\n",
    "print(strip_emails(test_email))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function to the column and assign it back to the column (it does not work inplace)\n",
    "X.email = X.email.apply(strip_emails)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is the email column going to be worth it?\n",
    "Let's take a look at this email column and decide if it could help us or not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many unique domains are there?\n",
    "counts = X.email.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(counts.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, it doesn't seem like this column will be very helpful as it's quite spread out. There are 490 unique values and no one value has a majority, so let's just leave it to the side for now. We can always come back to it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we make sure to copy it over and save it for later.\n",
    "emails = X.email.copy()\n",
    "X.drop('email', axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the IP Address\n",
    "We now need to split up the IP address, we will use Pandas's built in str method for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[['first_ip','second_ip','third_ip','fourth_ip']] = X.ip_address.str.split(pat=\".\", expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we cast the columns as floats\n",
    "X[['first_ip','second_ip','third_ip','fourth_ip']] = X[['first_ip','second_ip','third_ip','fourth_ip']].astype('float32')\n",
    "# we also drop the original column\n",
    "X.drop('ip_address', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Encoding\n",
    "\n",
    "Ok we are almost done, we just have to convert the gender column into something integer that we can use. We will use one-hot-encoding since gender is a categorical variable.\n",
    "\n",
    "Pandas has a `get_dummies()` function that will be very useful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.gender.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumbdumbs = pd.get_dummies(X['gender'])\n",
    "X= pd.concat((X,dumbdumbs), axis = 1)\n",
    "X.drop(['gender'], axis = 1, inplace=True)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing Stage 2 - Missing values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = imp.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= pd.DataFrame(X_, columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "Ok, now we have our customer data all ready to go. We have done all the hard work of preprocessing. Let's feed this data into our algorithms!\n",
    "\n",
    "We'll try two different clustering algorithms. K-means and DB-scan.\n",
    "\n",
    "\n",
    "Our goal is to cluster the data and learn what kinds of customers we have, are they related to one another at all? In order to do this we should try some clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster with  K-means\n",
    "\n",
    "Let's just make some clusters and evaluate it with their silhouette score. A reminder about the silhouette score\n",
    "\n",
    ">The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters. Negative values generally indicate that a sample has been assigned to the wrong cluster, as a different cluster is more similar.\n",
    "\n",
    "From : https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score\n",
    "\n",
    "Your Job:\n",
    "Run a k-means loop on clusters 2-n (you decide how many you want to try!) and see which # of clusters is best.\n",
    "\n",
    "You should scale the data first since we are hunting for clusters we definitely want the dimensions to be on the same scale (remember that distance means everything here!).\n",
    "\n",
    "#### Note:\n",
    "In an effort to slowly remove your training wheels, I have not important everything you need to accomplish your tasks here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import what you need\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale your data with a scaler of your choice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a k-means estimator and fit it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the silhouette score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a loop between 2-N (your choice for N!) and print this guy out.\n",
    "\n",
    "print(f\"The number of clusters is {} and the Silhouette score is {}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Discussion\n",
    "\n",
    "Ok, so we found the best number of clusters according to the silhouette score.  So what? I mean to say, what do we do with that? We know that according the silhouette score this is the best, but... even if that's correct, _now what_? How do we use these clusters to help us run our business?\n",
    "How can we learn what these clusters represent?\n",
    "\n",
    "I can think of one thing to check\n",
    "\n",
    "1. Look at the feature values of the cluster centroids.\n",
    "\n",
    "Recall that every cluster in kmeans has a centroid. That centroid is the very _center_ of the cluster, so we can look at the feature values of the centroids and see what they tell us. In theory the centroids represent the cluster (generally). \n",
    "\n",
    "We can look at the cluster centroids right now.  Let's examine then with a barplot.\n",
    "\n",
    "I'm going to give you some code help here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a kmeans cluster with desired number of components\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look for the centers of your clusters.  \n",
    "# Hint: your kmeans object has an attribute you'd want to use\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my gift to you, use it to plot your centers\n",
    "def plot_centroids(centers):\n",
    "    pd.DataFrame(centers, columns = X.columns).plot(kind = 'bar', figsize = (12,6))\n",
    "    plt.legend(loc='best', bbox_to_anchor=(0.8, 0.1, 0.5, 0.5)); # bbox is (x,y, width, height)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine your centroids\n",
    "\n",
    "What does your graph tell you?\n",
    "What features are the most important and contributing towards the clusters?\n",
    "\n",
    "Extra-Credit: Rerun your clustering algorithm with a _different_ scaling method and re-examine your centroid features. Do they change? Why? What does it mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DB-Scan\n",
    "\n",
    "Let's now try DB-scan. Remember we have three parameters we need to set\n",
    "1. `eps` the radius of our circle that we will search in\n",
    "2. `min_samples` the minimum number of samples we need to find within our radius to call it a cluster\n",
    "3. `metric` our distance metric.  Defaults to Euclidean (L2-norm).\n",
    "\n",
    "So, we get to fiddle with 3 parameters, but we don't have to worry about \"how many\" clusters to look for, db-scan will decide for us.\n",
    "\n",
    "Go ahead and run it!\n",
    "What is the best eps / min_samples ?\n",
    "What gets you the best silhouette score?\n",
    "\n",
    "In order to answer these questions we will need to figure out a few things\n",
    "\n",
    "1. run dbscan (that's pretty easy)\n",
    "2. how many clusters did it find?\n",
    "3. how do we get the labels (predictions) from the dbscan object?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run dbscan \n",
    "# pick an eps -- how might we do this? what range are your features in?\n",
    "# Your eps should be relevant to the feature space you exist in\n",
    "# pick min_samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### ok, now you fit a dbscan. \n",
    "1. how do we figure out how many clusters it found?\n",
    "DBscan has 3 main attributes, 1\n",
    "\n",
    ">**core_sample_indices_ndarray of shape (n_core_samples,)**\n",
    "Indices of core samples.\n",
    "\n",
    ">**components_ndarray of shape (n_core_samples, n_features)**\n",
    "Copy of each core sample found by training.\n",
    "\n",
    ">**labels_ndarray of shape (n_samples)**\n",
    "Cluster labels for each point in the dataset given to fit(). Noisy samples are given the label -1.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html\n",
    "\n",
    "Go ahead and look at those attributes. Print them out. Which one will help us figure out how many clusters DBscan found?\n",
    "\n",
    "Also pay attention to changing `eps`, if `eps` is too small what happens? Too large?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the Silhouette Score\n",
    "\n",
    "In order to perform the silhouette score we need the labels (that's pretty easy they are given to us), however we should _exclude_ the -1's because they represent points that are _not_ in a cluster. So they are basically discarded. We should probably collect those somewhere and else and see how large that number is. But it should not be part of the silhouette core.\n",
    "\n",
    "So\n",
    "1. seperate out the points which have the label `-1`\n",
    "\n",
    "Then after we have done that, we can calcluate the silhouette score easily. \n",
    "#### NOTE\n",
    "DBscan in scikit-learn is implemented with the notion of \"core samples\".  From the [documentation:](https://scikit-learn.org/stable/modules/clustering.html#dbscan)\n",
    ">More formally, we define a core sample as being a sample in the dataset such that there exist min_samples other samples within a distance of eps, which are defined as neighbors of the core sample. This tells us that the core sample is in a dense area of the vector space. A cluster is a set of core samples that can be built by recursively taking a core sample, finding all of its neighbors that are core samples, finding all of their neighbors that are core samples, and so on. **A cluster also has a set of non-core samples, which are samples that are neighbors of a core sample in the cluster but are not themselves core samples.** Intuitively, these samples are on the fringes of a cluster.\n",
    "\n",
    "This means that the attributes `components` and `core_sample_indices` will only return core samples, for us to get all the points _within_ the cluster we will have to rely on the labels\n",
    "\n",
    "### Number of clusters?\n",
    "But what about the # of clusters? How many did we find and how many in each cluster? We need to examine the labels again, but this time count the unique ones and also count how many of each unique we have.\n",
    "\n",
    "### Goal\n",
    "Run our little line of code, which gives both the score and # of clusters.\n",
    "\n",
    "`print(f\"The number of clusters is {num_clusters} and the Silhouette score is {silhouette_score(x_scaled, dbscan.labels_)}\")`\n",
    "\n",
    "\n",
    "Oh! we also should probably add\n",
    "`print(f\"The number of discarded points was {}\")`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove the -1's from the labels.\n",
    "## you also will need to remove the corresponding x_scaled samples so you can compute the silhouette score.\n",
    "# you can use numpy boolean indexing masks for this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ok if you have the labels and x's without -1's you can actually computer the Silhouette score now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## how many clusters are there though?\n",
    "## you need to figure out how many unique labels there are, and also it would be nice to know\n",
    "## how many points are in each cluster\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does it compare?\n",
    "\n",
    "Did your DBscan find similar clusters to k-means? Same number of clusters? More ? Less? What about the silhouette score? Better or worse?\n",
    "\n",
    "\n",
    "### Next we'd like to look at the centroids\n",
    "\n",
    "Actually, DBscan doesn't have centroids naturally the same way k-means does. But we can compute it ourselves.\n",
    "We would need to isolate the points of each group, and then just take the mean column wise, that would represent the centroid of each cluster.  You can do this with a numpy mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot your DBScan Centroids\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do your centroids compare?\n",
    "\n",
    "Did you find similar clusters? Dissimilar?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
