{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# NOTE! There are missing imports here, you will have to add to the imports as you go. \n",
    "# I suggest you add all your imports at the top of the notebook. (in this cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# California Housing with Linear Regression\n",
    "\n",
    "In this notebook we will work with a [California Housing Prices Data](https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html). \n",
    "We are going to use the [Scikit-Learn api](https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset) to automatically get it. \n",
    "\n",
    "Let's load the data and take a look at the information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing(as_frame=True) # as_frame will return pandas dataframes instead of numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn datasets normally have a .DESCR attribute, which will tell you about the dataset.\n",
    "print(housing.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's always a good idea to check the type of your variables (especially if you just loaded them automagically)\n",
    "type(housing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datatype `bunch` is used by sklearn to contain their datasets. The documentation is [here](https://scikit-learn.org/stable/modules/generated/sklearn.utils.Bunch.html). It's easiest to think of it as a dictionary which contains different elements of the dataset. So the most basic operation you will want to do is to print the `.keys()` in order to see what elements are available for access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set X to be the data. X is uppercase because it represents a matrix\n",
    "# since we used as_frame=True, housing.data is a pandas dataframe\n",
    "X = housing.data\n",
    "# set y to be the target labels. y is lowercase because it represents a vector.\n",
    "# since we used as_frame=True, housing.target is a pandas series\n",
    "y = housing.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the data\n",
    "\n",
    "The first step of any machine learning project is to explore the data a little bit. We already have a general idea because of the description given to us, but let's dig in a tiny bit more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the shape of your X matrix is always a good idea.\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 20640 samples (housing records) and 8 columns (features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the 20640 \"labels\" or put another way \"thing we are interested in predicting\".  In this case they are median housing prices, that is the price that the house sold for. From the documentation \"The target variable is the median house value for California districts,\n",
    "expressed in hundreds of thousands of dollars ($100,000).\"\n",
    "\n",
    "Let's put our data into a dataframe with pandas and examine the first 5 rows of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at the feature names from the dataframe\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we have glanced over the first 5 rows just to get a \"feel\" for the data, do we notice anything interesting? Well we see that the range of the values for each column is quite different, we need to deal with that. So we can add scaling our data to a list of things to do.  Before we run off and start scaling though, let's perform some basic exploratory data analysis. Our goal here is to visually and statisically examine the dataset and see what pops out at us. \n",
    "\n",
    "I think the very first step is to just look at the basic information of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the general information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info() # this is a simple function that will get us some basic info about our dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.info()` function will tell us a lot about our dataframe. We can see the RangeIndex, which basically tells us how many values are in the dataframe, further it will tell us what the datatype of the index is (if it's a datetime index for example). In this case we have a basic index, with 20640 rows. \n",
    "\n",
    "Next you can see the names of all the columns, and what `Dtype` they are. The datatype in pandas dataframes are heterogeneous, that means you can have different datatypes for different columns. This is just like your standard database - for the same performance reasons. Note that a numpy array _cannot_ have different datatypes for the different columns, this is a bonus feature that pandas gives us building on top of arrays.\n",
    "\n",
    "Note that `.info()` gives us a non-null count for each column. I find this helpful when scanning for missing values, but I don't only rely on it.\n",
    "Then we can also see the memory usage for our dataframe, this can be very helpful when you aren't sure what is hogging all the ram!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is one of many ways to check if there are any missing values in the dataset.\n",
    "X.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `.isnull().sum()` is one of many ways to check for missing values. I like it because it will show me the missing values column-wise, however if you have a lot of columns this will be truncated and you will have to do other ways. Note you can add an extra `.sum()` and get a single value of `na`'s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking deeper with EDA (exploratory data analysis)\n",
    "Ok, now we have look at \n",
    "The kinds of questions we want to be thinking about are:\n",
    "  *  Are any columns strongly correlated?\n",
    "  *  Do there appear to be columns that have wide ranges and potential outliers?\n",
    "  *  If we can understand the features, look for relationships that make logical sense (e.g., price of house vs size of house)\n",
    "\n",
    "We will look at raw numbers and visualizations now. Raw numbers will generally be summary statistics like the mean, standard deviation, max, min, etc. Visualizations will be things like pairplots, correlation matrices, etc.\n",
    "\n",
    "I don't think it really matters what you do first, you should just start with what you are more comfortable with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to start with the numbers, because that is the way that my brain works, I like to see numbers first. However, if you prefer plots first you can skip ahead and then return here.\n",
    "\n",
    "### `pandas.DataFrame.describe()`\n",
    "\n",
    "This function is very useful to get some quick intuitions for the dataset that you are working with.  Describe will give you thec ommon statistics of your matrix, the mean, std (standard deviation), min, max, and quantiles.  It's nice to scroll through and see if anything interesting pops out.  Looking through the output below, do you notice anything interesting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your thoughts here: \n",
    "\n",
    "---\n",
    "Your answer in here (double click to enter the markdown cell and edit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "--- \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the histograms\n",
    "\n",
    "Plot histograms for all the continous variables in the dataframe. Use the `.hist()` dataframe method.\n",
    "You should experiment with the bin size. Use the `bins=` keyword parameter to try different amounts of bins.\n",
    "I have given you some extra code below to space out the resulting plot a bit more nicely.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.hist(figsize=(12, 10), bins=30)\n",
    "plt.subplots_adjust(hspace=0.4, wspace=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What do you notice?\n",
    "\n",
    "What do the distributions tell you about each column?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your thoughts here: \n",
    "\n",
    "---\n",
    "Your answer in here (double click to enter the markdown cell and edit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "--- \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the histograms for the columns that seem only have one value.\n",
    "\n",
    "Note that some of the histograms seems to only have one column. That is because they are skewed, they have a lot of small values off to the right, they are so small you cannot see them. However, we can zoom in to see the skew.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# in order to limit the y-axis we need to expose the axes element\n",
    "# To get the axes, we will generate one with the subplots command.\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# select only the column you are interested in from X\n",
    "# note that we pass ax to the ax keyword. If you don't pass one, pandas will generate it's own.\n",
    "X['AveBedrms'].hist(bins=30, ax=ax)\n",
    "# now you can set the y_lim on the axes. \n",
    "# play with the top value to zoom in farther and farther\n",
    "ax.set_ylim(bottom=0, top=199)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we should also plot the histogram of the target variable!\n",
    "sns.displot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is that strange single bar above the 5?\n",
    "When we look at the distribution of the target variable, note that one value seems... a bit too high!\n",
    "In fact, there is nothing we can really \"do\" about this, except I personally wonder if they should have made their survey have some higher options on their scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the longitude and Latitude\n",
    "\n",
    "One fun thing we can do is to create a scatter plot of the longitude and latitude, while _coloring_ the datapoints according to a hue or gradient of colors. I think it's easiest to let the plot speak for itself so just go ahead and run the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.scatterplot(data=X, x=\"Longitude\", y=\"Latitude\",\n",
    "                size=y, hue=y,\n",
    "                palette=\"viridis\", alpha=0.5)\n",
    "plt.legend(title=\"MedHouseVal\", bbox_to_anchor=(1.05, 0.95),\n",
    "           loc=\"upper left\")\n",
    "plt.title(\"Median house value depending of their spatial location\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# It looks like a plot of California!\n",
    "As a California native I can tell you the two dominant yellow spots (expensive!) are the San Franciso bay area and the Los Angeles area. This data is actually pretty old, from before the silicon valley boom. So today I bet this map would be quite different. For those wonder what the small cluster of yellow is in the upper right - that's lake Tahoe - a popular ski resort and beautiful area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pairwise plot\n",
    "Next we will create what is known as a \"pairwise\" plot. The idea here is that we want to look at how the features are related to each other. Basically we want to see if they are correlated, if when one feature increases the other feature either goes up or down significantly. So we will do a grid plot where each feature will be compared with every other feature. \n",
    "\n",
    "The diagonal axis will produce histograms of the feature (basically the same thing we did early) because we are looking at how the feature interacts with itself.\n",
    "\n",
    "Using seaborn we can do an extra trick though - we can colorize the plots so that the hue (color pattern) corresponds to our target variable (the price of the house). This is really neat because we can not only see how the features interact with each other, but we get a _third_ dimension which is the target variable. So we in fact are seeing how each feature interacts with the target variable at the same time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the unwanted columns\n",
    "columns_drop = [\"Longitude\", \"Latitude\"]\n",
    "subset = X.drop(columns=columns_drop)\n",
    "subset[\"MedHouseVal\"]= y\n",
    "\n",
    "# Quantize the target and keep the midpoint for each interval\n",
    "# the reason we are doing this is because it makes the plotting readable. \n",
    "# it will essentially smooth the data, and squish down any wild outliers that cause the axis of the plots to be hard to read.\n",
    "# if you are curious you can comment out these lines and see the difference.\n",
    "subset[\"MedHouseVal\"] = pd.qcut(subset[\"MedHouseVal\"], 6, retbins=False)\n",
    "subset[\"MedHouseVal\"] = subset[\"MedHouseVal\"].apply(lambda x: x.mid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.pairplot(data=subset, hue=\"MedHouseVal\", palette=\"viridis\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are your thoughts on the pairplot. What do you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your thoughts here: \n",
    "\n",
    "---\n",
    "Your answer in here (double click to enter the markdown cell and edit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "--- \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Scikit-Learn API \n",
    "\n",
    "Many (most?) objects in scikit-learn have a `.fit` method which will tell the model to do whatever it was designed to learn.  From the objects that have a `.fit` method, you will get either a follow up `.transform` or `.predict` method.  Objects with `.transform` are transfromers (like the scaler we just used), their job is to learn some statistics from the data and transform it according to some logic (using the statistics that were learned).  Objects that have a `.predict` method are machine learning algorithms, and when you call `.fit` on them they are going to learn whatever rules / functions that the algorithm is designed to learn. When you call `.predict` with them, they will take as input a data point(s) and give you a prediction(s) for the input.\n",
    "\n",
    "So, in summary we frequently see three methods\n",
    "\n",
    "* `.fit` : this tells the object to learn\n",
    "* `.transform` : this tells the object to apply it's learning in the form a transformation to the input. This method _returns_ an object (the transformed data\n",
    "* `.predict` : this tells the object to make a prediction on the input, using whatever function the model learned (from the `.fit` call it made earlier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling our dataset\n",
    "\n",
    "Ok, we need to scale our dataset.  We learned about two choices so you can pick from two options!\n",
    "\n",
    "* StandardScaler\n",
    "* MinMaxScaler\n",
    "\n",
    "The order of operations goes like this:\n",
    "1. initialize a scaler\n",
    "2. call `fit` on our dataset\n",
    "3. transform our dataset and return that value to a new variable.\n",
    "\n",
    "Note: `.fit` and `.transform` are called seperately for some sneaky tricky reasons we will get into a bit later. For now just think to yourself about _why_ sklearn _might_ have seperate `.fit` and `.transform` functions on it's scalers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialize a scaler here\n",
    "\n",
    "scaler = # your choice of scaler here (check the imports to remember their names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fit your scaler here -- fitting happens \"in place\" which means it doesn't return anything\n",
    "# you will run something like this:\n",
    "# scaler.fit(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## transform our dataset now and return the value\n",
    "# you will need to run something like:\n",
    "# X_scaled = scaler.transform(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a model\n",
    "\n",
    "Ok it's time to train our model.  We will want to choose one of the three options we imported earlier.\n",
    "\n",
    "* LinearRegression\n",
    "* SGDRegressor\n",
    "* Ridge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a model here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your model here with .fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can take a look at the coefficients your model learned with model.coef_ and model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## umm... are we done?\n",
    "So we fit a model right? Now unlike our previous work, we can't really plot this model, it's not a line in 2d, it's a plane in 13d.  So what can we do?  We need to evaluate our model somehow! What should we do?\n",
    "How about we make a bunch of predictions and see what kind of accuracy it gets?  We can evaluate it's mean-squared error, the same metric we used to optimize it.\n",
    "\n",
    "# Evaluate our model\n",
    "\n",
    "Ok, let's use our trained model to make predictions, then we can evaluate those predictions against the real known `y` values. You will need to use your models `.predict()` function which needs some input to predict on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use your model to make predictions on the data\n",
    "# you will need to run something like:\n",
    "# y_pred = model.predict(X) - but you will need to use the model you trained above. \n",
    "# also, consider carefully what X data you should be using here \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok now we need to evaluate our predictions. We will use scikit-learns inbuilt mean squared error metric for this. It's very important that you pass your arguments correctly to the evaluation function, all scikit-learn metrics use `y_true, y_pred` format, which means pass the ground-truth first, followed by the prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now you need to evaluate your model.\n",
    "# you can use the mean squared error and the r2 score to do this.\n",
    "# you can import these from sklearn.metrics\n",
    "# remember that you need both predictions and ground truth to evaluate your model.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automate model production\n",
    "Let's build more models and automate the evaluation with a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# three models to try out - setting them all on defaults (you may need to add imports!)\n",
    "reg_ridge = Ridge()\n",
    "reg_SGD = SGDRegressor()\n",
    "reg_Linear = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put them in a list and loop over them\n",
    "regs= [reg_ridge, reg_SGD, reg_Linear]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write some code that will take your list and then iterate over each model\n",
    "# and print out the MSE and R2 score for each model.\n",
    "# you can use the __class__ attribute to automatically get the name of the model you are using.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Are we happy with this score?\n",
    "The R2 ranges between 0-1 , 1 being very correlated (and good in our case). \n",
    "For the MSE, it's instructive to look at the square root, which will be the real average error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt( ) #your mse value in here\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this a good MSE, Does the +/- seem good? Well it depends, we have to look at what that means in terms of the target variable.  When we plotted the distribution we saw that the target variable is between 0-5, so in general we are off by.. you tell me!\n",
    "\n",
    "In general, I don't think we'll be taking this model to the market to make a living as a real estate agent!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That's a wrap!\n",
    "\n",
    "So.. are we done now? Are you happy with our model?  What does our MSE even mean? Is it good? Is it bad?\n",
    "Does anything seem strange or wrong about how we evaluated our model?\n",
    "What is the funny feeling in our stomach from? What \"smells\" wrong here?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "679acf9e19cba1ecd7a688086508589c6b94a9f7e8fc94f0e948ec1d3905453e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
